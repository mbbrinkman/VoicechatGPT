<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="apple-mobile-web-app-title" content="Realtime API">
    <title>OpenAI Realtime API Console</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --bg-primary: #1a1d23;
            --bg-secondary: #252930;
            --bg-tertiary: #2d3139;
            --border-color: #3d4149;
            --text-primary: #e8e9ed;
            --text-secondary: #9195a0;
            --accent: #5b8bd4;
            --accent-hover: #7aa5e8;
            --success: #4a9d5f;
            --danger: #c94d4d;
            --warning: #d89b3d;
        }

        body {
            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, Roboto, Helvetica, Arial, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            min-height: 100vh;
            padding: env(safe-area-inset-top) env(safe-area-inset-right) env(safe-area-inset-bottom) env(safe-area-inset-left);
            line-height: 1.6;
        }

        .container {
            max-width: 720px;
            margin: 0 auto;
            padding: 20px;
            padding-bottom: 80px;
        }

        h1 {
            text-align: center;
            margin-bottom: 10px;
            font-size: 22px;
            font-weight: 400;
            letter-spacing: 0.5px;
            color: var(--text-primary);
        }

        .subtitle {
            text-align: center;
            font-size: 13px;
            color: var(--text-secondary);
            margin-bottom: 25px;
        }

        /* Tabs */
        .tabs {
            display: flex;
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 25px;
        }

        .tab {
            flex: 1;
            padding: 12px 20px;
            border: none;
            background: transparent;
            color: var(--text-secondary);
            font-size: 14px;
            font-weight: 500;
            cursor: pointer;
            border-bottom: 2px solid transparent;
            transition: all 0.2s;
        }

        .tab:hover {
            color: var(--text-primary);
        }

        .tab.active {
            color: var(--accent);
            border-bottom-color: var(--accent);
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        /* Status */
        .status {
            background: var(--bg-secondary);
            padding: 14px 18px;
            border: 1px solid var(--border-color);
            margin-bottom: 20px;
            font-size: 14px;
        }

        .status-indicator {
            display: inline-block;
            width: 8px;
            height: 8px;
            border-radius: 50%;
            margin-right: 10px;
        }

        .status-indicator.disconnected { background: var(--danger); }
        .status-indicator.connecting { background: var(--warning); }
        .status-indicator.connected { background: var(--success); }

        /* Controls */
        .control-group {
            background: var(--bg-secondary);
            padding: 16px;
            border: 1px solid var(--border-color);
            margin-bottom: 15px;
        }

        .control-group label {
            display: block;
            margin-bottom: 8px;
            font-weight: 500;
            font-size: 13px;
            color: var(--text-secondary);
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        input, select, textarea {
            width: 100%;
            padding: 10px 12px;
            border: 1px solid var(--border-color);
            background: var(--bg-tertiary);
            color: var(--text-primary);
            font-size: 14px;
            font-family: inherit;
            transition: border-color 0.2s;
        }

        input:focus, select:focus, textarea:focus {
            outline: none;
            border-color: var(--accent);
        }

        textarea {
            resize: vertical;
            min-height: 80px;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 13px;
        }

        .info-text {
            font-size: 12px;
            color: var(--text-secondary);
            margin-top: 6px;
            line-height: 1.5;
        }

        /* Buttons */
        button {
            width: 100%;
            padding: 12px 20px;
            border: 1px solid var(--border-color);
            background: var(--bg-secondary);
            color: var(--text-primary);
            font-size: 14px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            touch-action: manipulation;
        }

        button:hover:not(:disabled) {
            background: var(--bg-tertiary);
            border-color: var(--accent);
        }

        button:active:not(:disabled) {
            transform: translateY(1px);
        }

        button:disabled {
            opacity: 0.4;
            cursor: not-allowed;
        }

        .btn-primary {
            background: var(--accent);
            border-color: var(--accent);
        }

        .btn-primary:hover:not(:disabled) {
            background: var(--accent-hover);
            border-color: var(--accent-hover);
        }

        .btn-success {
            background: var(--success);
            border-color: var(--success);
        }

        .btn-success:hover:not(:disabled) {
            background: #5eb172;
            border-color: #5eb172;
        }

        .btn-danger {
            background: var(--danger);
            border-color: var(--danger);
        }

        .btn-danger:hover:not(:disabled) {
            background: #d66565;
            border-color: #d66565;
        }

        .btn-warning {
            background: var(--warning);
            border-color: var(--warning);
        }

        .btn-warning:hover:not(:disabled) {
            background: #e0a957;
            border-color: #e0a957;
        }

        .button-row {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
        }

        .button-row button {
            flex: 1;
        }

        /* Push to Talk */
        .ptt-button {
            height: 80px;
            font-size: 16px;
            margin: 20px 0;
            border: 2px solid var(--border-color);
        }

        .ptt-button.active {
            background: var(--danger);
            border-color: var(--danger);
        }

        /* Settings Sections */
        .settings-section {
            margin-bottom: 20px;
        }

        .settings-section h3 {
            margin-bottom: 15px;
            font-size: 14px;
            font-weight: 500;
            color: var(--text-secondary);
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .setting-item {
            margin-bottom: 15px;
        }

        .setting-item label {
            display: block;
            margin-bottom: 5px;
            font-size: 13px;
            color: var(--text-secondary);
        }

        /* Range Inputs */
        input[type="range"] {
            -webkit-appearance: none;
            appearance: none;
            background: var(--bg-tertiary);
            height: 4px;
            border: 1px solid var(--border-color);
        }

        input[type="range"]::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 16px;
            height: 16px;
            background: var(--accent);
            cursor: pointer;
            border: 1px solid var(--border-color);
        }

        input[type="range"]::-moz-range-thumb {
            width: 16px;
            height: 16px;
            background: var(--accent);
            cursor: pointer;
            border: 1px solid var(--border-color);
        }

        .range-value {
            display: inline-block;
            margin-left: 10px;
            font-weight: 600;
            color: var(--accent);
            font-family: 'Consolas', 'Monaco', monospace;
        }

        /* Checkbox */
        .checkbox-item {
            display: flex;
            align-items: center;
            gap: 10px;
            padding: 8px 0;
        }

        .checkbox-item input[type="checkbox"] {
            width: auto;
            height: 16px;
            accent-color: var(--accent);
        }

        .checkbox-item label {
            margin: 0 !important;
            font-size: 14px;
            color: var(--text-primary);
        }

        /* Video */
        .video-container {
            background: var(--bg-tertiary);
            border: 1px solid var(--border-color);
            margin-bottom: 20px;
            position: relative;
            display: none;
        }

        .video-container.active {
            display: block;
        }

        video {
            width: 100%;
            display: block;
        }

        .video-controls {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            padding: 10px;
            background: linear-gradient(to top, rgba(0,0,0,0.8), transparent);
            display: flex;
            gap: 10px;
        }

        .video-controls button {
            padding: 8px 16px;
            font-size: 13px;
        }

        /* Collapsible sections */
        .collapsible-header {
            background: var(--bg-secondary);
            padding: 12px 16px;
            border: 1px solid var(--border-color);
            cursor: pointer;
            margin-bottom: 10px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            transition: background 0.2s;
        }

        .collapsible-header:hover {
            background: var(--bg-tertiary);
        }

        .collapsible-header span {
            font-size: 13px;
            font-weight: 500;
            color: var(--text-primary);
        }

        .collapsible-content {
            display: none;
            padding-bottom: 10px;
        }

        .collapsible-content.open {
            display: block;
        }

        .collapse-icon {
            transition: transform 0.3s;
            color: var(--text-secondary);
            font-size: 12px;
        }

        .collapse-icon.open {
            transform: rotate(180deg);
        }

        /* Help Tab Styles */
        .help-section {
            margin-bottom: 30px;
        }

        .help-section h2 {
            font-size: 18px;
            font-weight: 500;
            margin-bottom: 12px;
            color: var(--text-primary);
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 8px;
        }

        .help-section h3 {
            font-size: 15px;
            font-weight: 500;
            margin: 18px 0 10px 0;
            color: var(--accent);
        }

        .help-section p {
            margin-bottom: 12px;
            color: var(--text-secondary);
            font-size: 14px;
            line-height: 1.7;
        }

        .help-section ul {
            margin-bottom: 12px;
            padding-left: 20px;
            color: var(--text-secondary);
            font-size: 14px;
        }

        .help-section li {
            margin-bottom: 6px;
            line-height: 1.6;
        }

        .help-section code {
            background: var(--bg-tertiary);
            padding: 2px 6px;
            border: 1px solid var(--border-color);
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 13px;
            color: var(--accent);
        }

        .help-section .note {
            background: var(--bg-secondary);
            border-left: 3px solid var(--accent);
            padding: 12px;
            margin: 15px 0;
            font-size: 13px;
            color: var(--text-secondary);
        }

        .help-section .warning {
            background: var(--bg-secondary);
            border-left: 3px solid var(--warning);
            padding: 12px;
            margin: 15px 0;
            font-size: 13px;
            color: var(--text-secondary);
        }

        @media (max-width: 400px) {
            .container { padding: 15px; padding-bottom: 80px; }
            h1 { font-size: 20px; }
            button { padding: 10px 16px; font-size: 13px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>OpenAI Realtime API Console</h1>
        <div class="subtitle">Speech-to-Speech Interface</div>

        <div class="status">
            <span class="status-indicator disconnected" id="statusIndicator"></span>
            <span id="statusText">Disconnected</span>
        </div>

        <!-- Tabs -->
        <div class="tabs">
            <button class="tab active" onclick="switchTab('chat')">Session</button>
            <button class="tab" onclick="switchTab('settings')">Configuration</button>
            <button class="tab" onclick="switchTab('help')">Documentation</button>
        </div>

        <!-- Chat Tab -->
        <div class="tab-content active" id="chatTab">
            <div class="control-group">
                <label for="apiKey">API Key</label>
                <input type="password" id="apiKey" placeholder="sk-...">
                <div class="info-text">
                    Stored in browser localStorage. Production deployments should use ephemeral tokens.
                </div>
            </div>

            <div class="button-row">
                <button onclick="connect()" class="btn-success" id="connectBtn">Connect</button>
                <button onclick="disconnect()" class="btn-danger" id="disconnectBtn" disabled>Disconnect</button>
            </div>

            <button class="ptt-button btn-warning" id="pttButton" disabled
                    onmousedown="startPushToTalk()"
                    onmouseup="stopPushToTalk()"
                    ontouchstart="startPushToTalk(event)"
                    ontouchend="stopPushToTalk(event)">
                PRESS AND HOLD TO SPEAK
            </button>

            <div class="button-row">
                <button onclick="interruptResponse()" class="btn-danger" id="interruptBtn" disabled>Interrupt</button>
            </div>

            <div class="button-row">
                <button onclick="startCamera()" class="btn-primary" id="cameraBtn">Camera</button>
                <button onclick="startScreenShare()" class="btn-primary" id="screenBtn">Screen Share</button>
            </div>

            <div class="video-container" id="videoContainer">
                <video id="localVideo" autoplay playsinline muted></video>
                <div class="video-controls">
                    <button onclick="flipCamera()" class="btn-primary" id="flipCameraBtn" style="display: none;">Flip</button>
                    <button onclick="stopVideo()" class="btn-danger">Stop</button>
                </div>
            </div>
        </div>

        <!-- Settings Tab -->
        <div class="tab-content" id="settingsTab">
            <!-- Model Settings -->
            <div class="settings-section">
                <div class="control-group">
                    <label for="model">Model</label>
                    <select id="model">
                        <option value="gpt-realtime" selected>gpt-realtime (Latest - Aug 2025)</option>
                        <option value="gpt-4o-realtime-preview-2025-06-03">gpt-4o-realtime-preview-2025-06-03</option>
                        <option value="gpt-4o-mini-realtime-preview-2024-12-17">gpt-4o-mini-realtime-preview (2024-12-17)</option>
                        <option value="gpt-4o-realtime-preview-2024-12-17">gpt-4o-realtime-preview (2024-12-17)</option>
                        <option value="gpt-4o-realtime-preview">gpt-4o-realtime-preview (Generic)</option>
                    </select>
                </div>

                <div class="control-group">
                    <label for="voice">Voice</label>
                    <select id="voice">
                        <option value="alloy">Alloy (Neutral, balanced)</option>
                        <option value="ash">Ash (Clear, precise)</option>
                        <option value="ballad">Ballad (Melodic, smooth)</option>
                        <option value="coral">Coral (Warm, friendly)</option>
                        <option value="echo">Echo (Resonant, deep)</option>
                        <option value="sage">Sage (Calm, thoughtful)</option>
                        <option value="shimmer" selected>Shimmer (Bright, energetic)</option>
                        <option value="verse">Verse (Versatile, expressive)</option>
                    </select>
                </div>

                <div class="control-group">
                    <label for="instructions">System Instructions</label>
                    <textarea id="instructions" placeholder="You are a helpful assistant...">You are a helpful assistant. Be concise and friendly.</textarea>
                </div>
            </div>

            <!-- Modalities -->
            <div class="collapsible-header" onclick="toggleCollapsible('modalities')">
                <span>Modalities</span>
                <span class="collapse-icon" id="modalities-icon">‚ñº</span>
            </div>
            <div class="collapsible-content" id="modalities-content">
                <div class="control-group">
                    <div class="checkbox-item">
                        <input type="checkbox" id="modalityText" checked>
                        <label for="modalityText">Text</label>
                    </div>
                    <div class="checkbox-item">
                        <input type="checkbox" id="modalityAudio" checked>
                        <label for="modalityAudio">Audio</label>
                    </div>
                </div>
            </div>

            <!-- Audio Formats -->
            <div class="collapsible-header" onclick="toggleCollapsible('audio')">
                <span>Audio Formats</span>
                <span class="collapse-icon" id="audio-icon">‚ñº</span>
            </div>
            <div class="collapsible-content" id="audio-content">
                <div class="control-group">
                    <label for="inputAudioFormat">Input Audio Format</label>
                    <select id="inputAudioFormat">
                        <option value="pcm16" selected>PCM16 (24kHz, 16-bit)</option>
                        <option value="g711_ulaw">G.711 Œº-law</option>
                        <option value="g711_alaw">G.711 A-law</option>
                    </select>
                </div>

                <div class="control-group">
                    <label for="outputAudioFormat">Output Audio Format</label>
                    <select id="outputAudioFormat">
                        <option value="pcm16" selected>PCM16 (24kHz, 16-bit)</option>
                        <option value="g711_ulaw">G.711 Œº-law</option>
                        <option value="g711_alaw">G.711 A-law</option>
                    </select>
                </div>
            </div>

            <!-- Turn Detection -->
            <div class="collapsible-header" onclick="toggleCollapsible('turndet')">
                <span>Turn Detection</span>
                <span class="collapse-icon" id="turndet-icon">‚ñº</span>
            </div>
            <div class="collapsible-content" id="turndet-content">
                <div class="control-group">
                    <label for="turnDetectionType">Type</label>
                    <select id="turnDetectionType" onchange="updateTurnDetectionUI()">
                        <option value="server_vad" selected>Server VAD</option>
                        <option value="semantic_vad">Semantic VAD</option>
                        <option value="none">None (Manual)</option>
                    </select>

                    <!-- Server VAD Settings -->
                    <div id="serverVadSettings">
                        <div class="setting-item">
                            <label>Threshold: <span class="range-value" id="vadThresholdValue">0.5</span></label>
                            <input type="range" id="vadThreshold" min="0" max="1" step="0.05" value="0.5">
                            <div class="info-text">Speech detection sensitivity (0.0-1.0)</div>
                        </div>

                        <div class="setting-item">
                            <label>Prefix Padding: <span class="range-value" id="prefixPaddingMsValue">300</span>ms</label>
                            <input type="range" id="prefixPaddingMs" min="0" max="1000" step="50" value="300">
                            <div class="info-text">Audio before speech detection</div>
                        </div>

                        <div class="setting-item">
                            <label>Silence Duration: <span class="range-value" id="silenceDurationMsValue">800</span>ms</label>
                            <input type="range" id="silenceDurationMs" min="100" max="3000" step="100" value="800">
                            <div class="info-text">Silence before turn ends</div>
                        </div>
                    </div>

                    <!-- Semantic VAD Settings -->
                    <div id="semanticVadSettings" style="display: none;">
                        <div class="setting-item">
                            <label for="eagerness">Eagerness</label>
                            <select id="eagerness">
                                <option value="auto" selected>Auto</option>
                                <option value="low">Low</option>
                                <option value="medium">Medium</option>
                                <option value="high">High</option>
                            </select>
                            <div class="info-text">How eagerly the model detects turn completion</div>
                        </div>

                        <div class="checkbox-item">
                            <input type="checkbox" id="createResponse" checked>
                            <label for="createResponse">Auto-create Response</label>
                        </div>

                        <div class="checkbox-item">
                            <input type="checkbox" id="interruptResponse" checked>
                            <label for="interruptResponse">Allow Interrupt Response</label>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Noise Reduction -->
            <div class="collapsible-header" onclick="toggleCollapsible('noise')">
                <span>Input Audio Noise Reduction</span>
                <span class="collapse-icon" id="noise-icon">‚ñº</span>
            </div>
            <div class="collapsible-content" id="noise-content">
                <div class="control-group">
                    <div class="checkbox-item">
                        <input type="checkbox" id="enableNoiseReduction">
                        <label for="enableNoiseReduction">Enable Noise Reduction</label>
                    </div>

                    <div id="noiseReductionSettings" style="display: none;">
                        <label for="noiseReductionType">Mode</label>
                        <select id="noiseReductionType">
                            <option value="near_field" selected>Near Field (headset/close mic)</option>
                            <option value="far_field">Far Field (laptop/room mic)</option>
                        </select>
                    </div>
                </div>
            </div>

            <!-- Response Generation -->
            <div class="collapsible-header" onclick="toggleCollapsible('response')">
                <span>Response Generation</span>
                <span class="collapse-icon" id="response-icon">‚ñº</span>
            </div>
            <div class="collapsible-content" id="response-content">
                <div class="control-group">
                    <div class="setting-item">
                        <label>Temperature: <span class="range-value" id="temperatureValue">0.8</span></label>
                        <input type="range" id="temperature" min="0.6" max="1.2" step="0.05" value="0.8">
                        <div class="info-text">Randomness (0.6-1.2, recommended: 0.8)</div>
                    </div>

                    <div class="setting-item">
                        <label>Max Response Tokens: <span class="range-value" id="maxResponseTokensValue">4096</span></label>
                        <input type="range" id="maxResponseTokens" min="256" max="4096" step="256" value="4096">
                        <div class="info-text">Maximum output length</div>
                    </div>
                </div>
            </div>

            <!-- Input Audio Transcription -->
            <div class="collapsible-header" onclick="toggleCollapsible('transcription')">
                <span>Input Audio Transcription</span>
                <span class="collapse-icon" id="transcription-icon">‚ñº</span>
            </div>
            <div class="collapsible-content" id="transcription-content">
                <div class="control-group">
                    <div class="checkbox-item">
                        <input type="checkbox" id="enableTranscription">
                        <label for="enableTranscription">Enable Transcription</label>
                    </div>

                    <div id="transcriptionSettings" style="display: none;">
                        <div class="setting-item">
                            <label for="transcriptionModel">Model</label>
                            <input type="text" id="transcriptionModel" value="whisper-1">
                        </div>
                    </div>
                </div>
            </div>

            <!-- Tools -->
            <div class="collapsible-header" onclick="toggleCollapsible('tools')">
                <span>Tools & Function Calling</span>
                <span class="collapse-icon" id="tools-icon">‚ñº</span>
            </div>
            <div class="collapsible-content" id="tools-content">
                <div class="control-group">
                    <label for="toolChoice">Tool Choice</label>
                    <select id="toolChoice">
                        <option value="auto" selected>Auto</option>
                        <option value="none">None</option>
                        <option value="required">Required</option>
                    </select>
                    <div class="info-text">No tools configured. Add via session.update.</div>
                </div>
            </div>
        </div>

        <!-- Help Tab -->
        <div class="tab-content" id="helpTab">
            <div class="help-section">
                <h2>Overview</h2>
                <p>This console provides a browser-based interface to the OpenAI Realtime API, enabling speech-to-speech conversations with minimal latency. The Realtime API supports both audio and text modalities, voice activity detection, and function calling.</p>

                <div class="note">
                    <strong>Note:</strong> This implementation uses client-side API keys for convenience. Production applications should use ephemeral tokens generated by your backend server.
                </div>
            </div>

            <div class="help-section">
                <h2>Getting Started</h2>
                <h3>1. API Key</h3>
                <p>Enter your OpenAI API key in the Session tab. The key is stored in browser localStorage and never leaves your device except when sent to OpenAI's servers.</p>

                <h3>2. Connect</h3>
                <p>Click "Connect" to establish a WebSocket connection to the Realtime API. The status indicator will turn green when connected.</p>

                <h3>3. Speaking</h3>
                <p>Use the "PRESS AND HOLD TO SPEAK" button for push-to-talk interaction. Release the button to submit your audio. With Server VAD or Semantic VAD enabled, you can also speak continuously without holding the button.</p>
            </div>

            <div class="help-section">
                <h2>Configuration Reference</h2>

                <h3>Model Selection</h3>
                <ul>
                    <li><code>gpt-realtime</code> - Latest production model (August 2025), recommended for most use cases</li>
                    <li><code>gpt-4o-realtime-preview-2025-06-03</code> - June 2025 preview with refined voice palette</li>
                    <li><code>gpt-4o-mini-realtime-preview-2024-12-17</code> - Smaller, faster variant for low-latency applications</li>
                    <li><code>gpt-4o-realtime-preview-2024-12-17</code> - December 2024 preview version</li>
                    <li><code>gpt-4o-realtime-preview</code> - Generic alias for latest preview</li>
                </ul>

                <h3>Voice Options</h3>
                <p>The API provides eight distinct voices with different characteristics:</p>
                <ul>
                    <li><strong>Alloy</strong> - Neutral and balanced tone</li>
                    <li><strong>Ash</strong> - Clear and precise articulation</li>
                    <li><strong>Ballad</strong> - Melodic and smooth delivery</li>
                    <li><strong>Coral</strong> - Warm and friendly demeanor</li>
                    <li><strong>Echo</strong> - Resonant and deep voice</li>
                    <li><strong>Sage</strong> - Calm and thoughtful presentation</li>
                    <li><strong>Shimmer</strong> - Bright and energetic expression</li>
                    <li><strong>Verse</strong> - Versatile and expressive range</li>
                </ul>

                <h3>Modalities</h3>
                <p>Control which input/output modes the model uses:</p>
                <ul>
                    <li><strong>Text</strong> - Enable text-based responses</li>
                    <li><strong>Audio</strong> - Enable audio-based responses</li>
                </ul>
                <p>Both can be enabled simultaneously for multimodal interaction.</p>

                <h3>Audio Formats</h3>
                <ul>
                    <li><strong>PCM16</strong> - Uncompressed 24kHz 16-bit audio (highest quality, ~384 kbps)</li>
                    <li><strong>G.711 Œº-law</strong> - Compressed telephony format (lower bandwidth)</li>
                    <li><strong>G.711 A-law</strong> - Alternative telephony compression standard</li>
                </ul>

                <h3>Turn Detection</h3>
                <p>Controls how the system detects when you've finished speaking:</p>

                <p><strong>Server VAD (Voice Activity Detection):</strong></p>
                <ul>
                    <li><strong>Threshold</strong> (0.0-1.0) - Speech detection sensitivity. Higher values require clearer speech signal.</li>
                    <li><strong>Prefix Padding</strong> (0-1000ms) - Audio captured before speech is detected, ensuring no words are cut off.</li>
                    <li><strong>Silence Duration</strong> (100-3000ms) - How long the system waits in silence before ending your turn. Default 800ms.</li>
                </ul>

                <p><strong>Semantic VAD:</strong></p>
                <ul>
                    <li><strong>Eagerness</strong> - How quickly the model decides you've finished speaking based on semantic meaning:
                        <ul>
                            <li><em>Auto</em> - Model decides automatically</li>
                            <li><em>Low</em> - Waits longer before deciding turn is complete</li>
                            <li><em>Medium</em> - Balanced approach</li>
                            <li><em>High</em> - Responds quickly to complete thoughts</li>
                        </ul>
                    </li>
                    <li><strong>Auto-create Response</strong> - Automatically generate response when turn is detected</li>
                    <li><strong>Allow Interrupt Response</strong> - Allow new speech to interrupt ongoing response</li>
                </ul>

                <p><strong>None (Manual):</strong></p>
                <p>Disables automatic turn detection. Use push-to-talk button to manually control when to submit audio.</p>

                <h3>Input Audio Noise Reduction</h3>
                <ul>
                    <li><strong>Near Field</strong> - Optimized for close-talking microphones (headsets, earbuds)</li>
                    <li><strong>Far Field</strong> - Optimized for distant microphones (laptop, conference room)</li>
                </ul>

                <h3>Response Generation</h3>
                <ul>
                    <li><strong>Temperature</strong> (0.6-1.2) - Controls randomness. 0.8 is recommended. Lower values are more deterministic, higher values more creative.</li>
                    <li><strong>Max Response Tokens</strong> (256-4096) - Maximum length of generated response. Higher values allow longer responses but may increase latency.</li>
                </ul>

                <h3>Input Audio Transcription</h3>
                <p>When enabled, uses Whisper to provide text transcripts of your speech. The Realtime API processes audio natively without transcription, but enabling this provides conversation.item.audio_transcription.completed events for logging or display.</p>

                <h3>Tools & Function Calling</h3>
                <ul>
                    <li><strong>Auto</strong> - Model decides whether to call functions</li>
                    <li><strong>None</strong> - Disable function calling</li>
                    <li><strong>Required</strong> - Force the model to call a function</li>
                </ul>
                <p>Functions must be defined separately via session.update events.</p>
            </div>

            <div class="help-section">
                <h2>Limitations</h2>
                <ul>
                    <li>Maximum session duration: 60 minutes (extended from 15 minutes in earlier versions)</li>
                    <li>Maximum context: 128,000 tokens</li>
                    <li>Audio consumption: ~800 tokens per minute</li>
                    <li>Voice cannot be changed after initial assistant response in a conversation</li>
                    <li>No structured output support</li>
                </ul>
            </div>

            <div class="help-section">
                <h2>Browser Compatibility</h2>
                <p>This console requires:</p>
                <ul>
                    <li>WebSocket support (all modern browsers)</li>
                    <li>Web Audio API (Chrome, Firefox, Safari, Edge)</li>
                    <li>MediaDevices API for microphone access</li>
                    <li>getUserMedia support for camera/screen sharing</li>
                </ul>

                <div class="warning">
                    <strong>iOS Safari:</strong> Some features require user gesture to unlock audio context. The app handles this automatically on first touch/click.
                </div>
            </div>

            <div class="help-section">
                <h2>Security Considerations</h2>
                <div class="warning">
                    <strong>Production Deployment:</strong> This implementation exposes your API key in the browser. For production applications:
                    <ol>
                        <li>Generate ephemeral tokens from your backend server</li>
                        <li>Use session authentication instead of API keys</li>
                        <li>Implement rate limiting and abuse prevention</li>
                        <li>Enable content moderation if accepting user input</li>
                    </ol>
                </div>
            </div>

            <div class="help-section">
                <h2>Troubleshooting</h2>
                <h3>Connection Issues</h3>
                <ul>
                    <li>Verify API key is valid and has sufficient quota</li>
                    <li>Check browser console for WebSocket errors</li>
                    <li>Ensure firewall allows WebSocket connections to api.openai.com</li>
                </ul>

                <h3>Audio Problems</h3>
                <ul>
                    <li>Grant microphone permissions when prompted</li>
                    <li>Check system audio input settings</li>
                    <li>Try different audio formats (PCM16 recommended)</li>
                    <li>Adjust VAD threshold if speech not detected</li>
                </ul>

                <h3>No Response</h3>
                <ul>
                    <li>Ensure at least one modality (text or audio) is enabled</li>
                    <li>Check turn detection is configured appropriately</li>
                    <li>Verify silence duration is not too short</li>
                    <li>Look for error events in browser console</li>
                </ul>
            </div>

            <div class="help-section">
                <h2>Additional Resources</h2>
                <ul>
                    <li><a href="https://platform.openai.com/docs/guides/realtime" target="_blank" style="color: var(--accent);">Official Realtime API Documentation</a></li>
                    <li><a href="https://platform.openai.com/docs/api-reference/realtime" target="_blank" style="color: var(--accent);">API Reference</a></li>
                    <li><a href="https://developers.openai.com/blog/realtime-api/" target="_blank" style="color: var(--accent);">Developer Notes</a></li>
                </ul>
            </div>
        </div>
    </div>

    <script>
        // Global state
        let ws = null;
        let audioContext = null;
        let audioStream = null;
        let mediaRecorder = null;
        let isRecording = false;
        let isPushToTalkActive = false;
        let mediaStream = null;
        let audioQueue = [];
        let isPlaying = false;
        let currentAudioSource = null;
        let audioContextUnlocked = false;
        let audioProcessor = null;
        let audioSource = null;
        let currentFacingMode = 'user'; // 'user' for front camera, 'environment' for rear

        // DOM elements
        const apiKeyInput = document.getElementById('apiKey');
        const statusIndicator = document.getElementById('statusIndicator');
        const statusText = document.getElementById('statusText');
        const connectBtn = document.getElementById('connectBtn');
        const disconnectBtn = document.getElementById('disconnectBtn');
        const pttButton = document.getElementById('pttButton');
        const interruptBtn = document.getElementById('interruptBtn');
        const videoContainer = document.getElementById('videoContainer');
        const localVideo = document.getElementById('localVideo');

        // Load API key
        window.addEventListener('load', () => {
            const savedKey = localStorage.getItem('openai_api_key');
            if (savedKey) apiKeyInput.value = savedKey;

            setupRangeInputs();
            setupCheckboxListeners();

            document.addEventListener('touchstart', unlockAudioContext, { once: true });
            document.addEventListener('click', unlockAudioContext, { once: true });
        });

        apiKeyInput.addEventListener('change', () => {
            localStorage.setItem('openai_api_key', apiKeyInput.value);
        });

        function unlockAudioContext() {
            if (!audioContextUnlocked && audioContext) {
                const buffer = audioContext.createBuffer(1, 1, 22050);
                const source = audioContext.createBufferSource();
                source.buffer = buffer;
                source.connect(audioContext.destination);
                source.start(0);
                audioContextUnlocked = true;
                console.log('üîì Audio context unlocked');
            }
        }

        function switchTab(tab) {
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            document.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));

            if (tab === 'chat') {
                document.querySelectorAll('.tab')[0].classList.add('active');
                document.getElementById('chatTab').classList.add('active');
            } else if (tab === 'settings') {
                document.querySelectorAll('.tab')[1].classList.add('active');
                document.getElementById('settingsTab').classList.add('active');
            } else if (tab === 'help') {
                document.querySelectorAll('.tab')[2].classList.add('active');
                document.getElementById('helpTab').classList.add('active');
            }
        }

        function toggleCollapsible(id) {
            const content = document.getElementById(id + '-content');
            const icon = document.getElementById(id + '-icon');
            content.classList.toggle('open');
            icon.classList.toggle('open');
        }

        function setupRangeInputs() {
            const ranges = [
                'vadThreshold', 'prefixPaddingMs', 'silenceDurationMs',
                'temperature', 'maxResponseTokens'
            ];

            ranges.forEach(id => {
                const input = document.getElementById(id);
                const valueSpan = document.getElementById(id + 'Value');
                if (input && valueSpan) {
                    input.addEventListener('input', () => {
                        valueSpan.textContent = input.value;
                    });
                }
            });
        }

        function setupCheckboxListeners() {
            document.getElementById('enableNoiseReduction').addEventListener('change', (e) => {
                document.getElementById('noiseReductionSettings').style.display =
                    e.target.checked ? 'block' : 'none';
            });

            document.getElementById('enableTranscription').addEventListener('change', (e) => {
                document.getElementById('transcriptionSettings').style.display =
                    e.target.checked ? 'block' : 'none';
            });
        }

        function updateTurnDetectionUI() {
            const type = document.getElementById('turnDetectionType').value;
            document.getElementById('serverVadSettings').style.display =
                type === 'server_vad' ? 'block' : 'none';
            document.getElementById('semanticVadSettings').style.display =
                type === 'semantic_vad' ? 'block' : 'none';
        }

        function updateStatus(status, text) {
            statusIndicator.className = `status-indicator ${status}`;
            statusText.textContent = text;
        }

        function getSessionConfig() {
            const modalities = [];
            if (document.getElementById('modalityText').checked) modalities.push('text');
            if (document.getElementById('modalityAudio').checked) modalities.push('audio');

            const config = {
                modalities: modalities,
                instructions: document.getElementById('instructions').value,
                voice: document.getElementById('voice').value,
                input_audio_format: document.getElementById('inputAudioFormat').value,
                output_audio_format: document.getElementById('outputAudioFormat').value,
                temperature: parseFloat(document.getElementById('temperature').value),
                max_response_output_tokens: parseInt(document.getElementById('maxResponseTokens').value),
                tool_choice: document.getElementById('toolChoice').value
            };

            // Turn detection
            const turnDetectionType = document.getElementById('turnDetectionType').value;
            if (turnDetectionType === 'server_vad') {
                config.turn_detection = {
                    type: 'server_vad',
                    threshold: parseFloat(document.getElementById('vadThreshold').value),
                    prefix_padding_ms: parseInt(document.getElementById('prefixPaddingMs').value),
                    silence_duration_ms: parseInt(document.getElementById('silenceDurationMs').value)
                };
            } else if (turnDetectionType === 'semantic_vad') {
                config.turn_detection = {
                    type: 'semantic_vad',
                    eagerness: document.getElementById('eagerness').value,
                    create_response: document.getElementById('createResponse').checked,
                    interrupt_response: document.getElementById('interruptResponse').checked
                };
            } else {
                config.turn_detection = null;
            }

            // Noise reduction
            if (document.getElementById('enableNoiseReduction').checked) {
                config.input_audio_noise_reduction = {
                    type: document.getElementById('noiseReductionType').value
                };
            }

            // Transcription
            if (document.getElementById('enableTranscription').checked) {
                config.input_audio_transcription = {
                    model: document.getElementById('transcriptionModel').value
                };
            }

            return config;
        }

        async function connect() {
            const apiKey = apiKeyInput.value.trim();
            if (!apiKey) {
                alert('Please enter your OpenAI API key');
                return;
            }

            try {
                updateStatus('connecting', 'Connecting...');
                connectBtn.disabled = true;

                // Initialize audio context
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)({
                        sampleRate: 24000,
                        latencyHint: 'interactive'
                    });
                }

                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Get microphone
                audioStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 24000,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });

                // Connect WebSocket
                const model = document.getElementById('model').value;
                const wsUrl = `wss://api.openai.com/v1/realtime?model=${model}`;
                const protocols = ["realtime", `openai-insecure-api-key.${apiKey}`, "openai-beta.realtime-v1"];

                ws = new WebSocket(wsUrl, protocols);

                ws.onopen = () => {
                    console.log('‚úÖ WebSocket connected');

                    const sessionConfig = getSessionConfig();
                    console.log('üì§ Session config:', sessionConfig);

                    ws.send(JSON.stringify({
                        type: 'session.update',
                        session: sessionConfig
                    }));

                    updateStatus('connected', 'Connected');
                    connectBtn.disabled = true;
                    disconnectBtn.disabled = false;
                    pttButton.disabled = false;
                    interruptBtn.disabled = false;

                    // Auto-start recording if VAD enabled
                    if (document.getElementById('turnDetectionType').value !== 'none') {
                        startContinuousRecording();
                    }
                };

                ws.onmessage = async (event) => {
                    try {
                        const message = JSON.parse(event.data);
                        console.log('üì®', message.type);

                        switch (message.type) {
                            case 'session.created':
                            case 'session.updated':
                                console.log('‚úÖ Session:', message.session);
                                break;

                            case 'response.audio.delta':
                                if (message.delta) await playAudioDelta(message.delta);
                                break;

                            case 'input_audio_buffer.speech_started':
                                statusText.textContent = 'Listening...';
                                break;

                            case 'input_audio_buffer.speech_stopped':
                                statusText.textContent = 'Processing...';
                                break;

                            case 'conversation.item.input_audio_transcription.completed':
                                console.log('üìù You:', message.transcript);
                                break;

                            case 'response.audio_transcript.done':
                                console.log('üìù Assistant:', message.transcript);
                                break;

                            case 'response.text.done':
                                console.log('üìù Assistant text:', message.text);
                                break;

                            case 'response.created':
                                statusText.textContent = 'Responding...';
                                break;

                            case 'response.done':
                                statusText.textContent = 'Connected';
                                break;

                            case 'error':
                                console.error('‚ùå Error:', message.error);
                                statusText.textContent = `Error: ${message.error.type}`;

                                let errorMsg = message.error.message || 'Unknown error';
                                if (message.error.code === 'invalid_api_key') {
                                    errorMsg = 'Invalid API key';
                                } else if (message.error.code === 'insufficient_quota') {
                                    errorMsg = 'Insufficient quota';
                                }
                                alert(`Error: ${errorMsg}`);
                                break;
                        }
                    } catch (error) {
                        console.error('‚ùå Parse error:', error);
                    }
                };

                ws.onerror = (error) => {
                    console.error('‚ùå WebSocket error:', error);
                    updateStatus('disconnected', 'Connection error');
                };

                ws.onclose = () => {
                    console.log('WebSocket closed');
                    disconnect();
                };

            } catch (error) {
                console.error('‚ùå Connection failed:', error);
                alert(`Failed: ${error.message}`);
                updateStatus('disconnected', 'Failed');
                connectBtn.disabled = false;
            }
        }

        function disconnect() {
            if (ws) {
                ws.close();
                ws = null;
            }

            if (audioProcessor) {
                audioProcessor.disconnect();
                audioProcessor = null;
            }

            if (audioSource) {
                audioSource.disconnect();
                audioSource = null;
            }

            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;
            }

            updateStatus('disconnected', 'Disconnected');
            connectBtn.disabled = false;
            disconnectBtn.disabled = true;
            pttButton.disabled = true;
            interruptBtn.disabled = true;
            isRecording = false;
        }

        async function startContinuousRecording() {
            if (!audioStream || isRecording) return;

            try {
                audioSource = audioContext.createMediaStreamSource(audioStream);
                audioProcessor = audioContext.createScriptProcessor(2048, 1, 1);

                audioProcessor.onaudioprocess = (e) => {
                    if (!ws || ws.readyState !== WebSocket.OPEN) return;

                    const inputData = e.inputBuffer.getChannelData(0);
                    const pcm16 = floatTo16BitPCM(inputData);
                    const base64Audio = arrayBufferToBase64(pcm16);

                    try {
                        ws.send(JSON.stringify({
                            type: 'input_audio_buffer.append',
                            audio: base64Audio
                        }));
                    } catch (error) {
                        console.error('‚ùå Send error:', error);
                    }
                };

                audioSource.connect(audioProcessor);
                audioProcessor.connect(audioContext.destination);
                isRecording = true;

                console.log('‚úÖ Recording started');
            } catch (error) {
                console.error('‚ùå Recording error:', error);
                alert('Failed to start recording: ' + error.message);
            }
        }

        function startPushToTalk(event) {
            if (event) event.preventDefault();
            if (!ws || ws.readyState !== WebSocket.OPEN || isPushToTalkActive) return;

            isPushToTalkActive = true;
            pttButton.classList.add('active');
            pttButton.textContent = 'RECORDING...';

            ws.send(JSON.stringify({ type: 'input_audio_buffer.clear' }));
            startContinuousRecording();
        }

        function stopPushToTalk(event) {
            if (event) event.preventDefault();
            if (!isPushToTalkActive) return;

            isPushToTalkActive = false;
            pttButton.classList.remove('active');
            pttButton.textContent = 'PRESS AND HOLD TO SPEAK';

            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: 'input_audio_buffer.commit' }));
                ws.send(JSON.stringify({ type: 'response.create' }));
            }
        }

        function interruptResponse() {
            if (!ws || ws.readyState !== WebSocket.OPEN) return;

            if (currentAudioSource) {
                currentAudioSource.stop();
                currentAudioSource = null;
            }
            audioQueue = [];
            isPlaying = false;

            ws.send(JSON.stringify({ type: 'response.cancel' }));
            ws.send(JSON.stringify({ type: 'input_audio_buffer.clear' }));

            console.log('‚èπÔ∏è Interrupted');
        }

        async function playAudioDelta(base64Audio) {
            try {
                const audioData = base64ToArrayBuffer(base64Audio);
                const float32Data = pcm16ToFloat32(new Int16Array(audioData));

                const audioBuffer = audioContext.createBuffer(1, float32Data.length, 24000);
                audioBuffer.getChannelData(0).set(float32Data);

                audioQueue.push(audioBuffer);

                if (!isPlaying) playNextInQueue();
            } catch (error) {
                console.error('‚ùå Audio playback error:', error);
            }
        }

        function playNextInQueue() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                return;
            }

            isPlaying = true;
            const audioBuffer = audioQueue.shift();

            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);

            source.onended = () => {
                currentAudioSource = null;
                playNextInQueue();
            };

            currentAudioSource = source;
            source.start(0);
        }

        // Audio conversion utilities
        function floatTo16BitPCM(float32Array) {
            const buffer = new ArrayBuffer(float32Array.length * 2);
            const view = new DataView(buffer);
            let offset = 0;

            for (let i = 0; i < float32Array.length; i++, offset += 2) {
                const s = Math.max(-1, Math.min(1, float32Array[i]));
                view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
            }

            return buffer;
        }

        function pcm16ToFloat32(int16Array) {
            const float32Array = new Float32Array(int16Array.length);
            for (let i = 0; i < int16Array.length; i++) {
                float32Array[i] = int16Array[i] / (int16Array[i] < 0 ? 0x8000 : 0x7FFF);
            }
            return float32Array;
        }

        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.byteLength; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }

        function base64ToArrayBuffer(base64) {
            const binary = atob(base64);
            const bytes = new Uint8Array(binary.length);
            for (let i = 0; i < binary.length; i++) {
                bytes[i] = binary.charCodeAt(i);
            }
            return bytes.buffer;
        }

        // Video/Screen sharing
        async function startCamera(facingMode = null) {
            try {
                // Stop existing stream if any
                if (mediaStream) {
                    mediaStream.getTracks().forEach(track => track.stop());
                }

                // Use provided facing mode or current
                if (facingMode) {
                    currentFacingMode = facingMode;
                }

                mediaStream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        facingMode: currentFacingMode,
                        width: 1280,
                        height: 720
                    },
                    audio: false
                });
                localVideo.srcObject = mediaStream;
                videoContainer.classList.add('active');

                // Show flip button for camera (not screen share)
                document.getElementById('flipCameraBtn').style.display = 'block';
            } catch (error) {
                console.error('‚ùå Camera error:', error);
                alert('Failed to access camera: ' + error.message);
            }
        }

        async function flipCamera() {
            // Toggle between front and rear camera
            const newFacingMode = currentFacingMode === 'user' ? 'environment' : 'user';
            await startCamera(newFacingMode);
        }

        async function startScreenShare() {
            try {
                // Stop existing stream if any
                if (mediaStream) {
                    mediaStream.getTracks().forEach(track => track.stop());
                }

                mediaStream = await navigator.mediaDevices.getDisplayMedia({
                    video: { width: 1920, height: 1080 },
                    audio: false
                });
                localVideo.srcObject = mediaStream;
                videoContainer.classList.add('active');

                // Hide flip button for screen share
                document.getElementById('flipCameraBtn').style.display = 'none';

                mediaStream.getVideoTracks()[0].onended = () => stopVideo();
            } catch (error) {
                console.error('‚ùå Screen share error:', error);
                alert('Failed to share screen: ' + error.message);
            }
        }

        function stopVideo() {
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            localVideo.srcObject = null;
            videoContainer.classList.remove('active');

            // Hide flip button when video is stopped
            document.getElementById('flipCameraBtn').style.display = 'none';
        }

        // iOS background audio support
        if ('mediaSession' in navigator) {
            navigator.mediaSession.metadata = new MediaMetadata({
                title: 'OpenAI Realtime API',
                artist: 'Speech-to-Speech Console',
                album: 'Voice Interface',
            });
        }

        // Wake lock
        let wakeLock = null;
        async function requestWakeLock() {
            try {
                if ('wakeLock' in navigator) {
                    wakeLock = await navigator.wakeLock.request('screen');
                    console.log('üîí Wake lock acquired');
                }
            } catch (err) {
                console.error('‚ùå Wake lock error:', err);
            }
        }

        connectBtn.addEventListener('click', requestWakeLock);

        document.addEventListener('visibilitychange', async () => {
            if (wakeLock !== null && document.visibilityState === 'visible') {
                await requestWakeLock();
            }
        });
    </script>
</body>
</html>
